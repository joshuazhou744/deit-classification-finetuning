{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891c8932-d15d-4cc4-9ef7-e6a4b594a1b0",
   "metadata": {},
   "source": [
    "# **Fine Tune a Vision Transformer for Image Classification with LoRA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2970ee-28a8-44bb-8f76-b4866f5eb7af",
   "metadata": {},
   "source": [
    "In this notebook we'll explore how to fine-tune a **Vision Transformer (ViT)** model efficiently using **Low-Rank Adaptation (LoRA)**. The goal is to adapt a pretrained image classification model to recognize new classes from a small dataset without requiring **massive compute resources** from training entirely new parameters. The process is also optimized for CPU so basically any machine can run this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad5b42-314b-46d3-90cd-934f0fd82871",
   "metadata": {},
   "source": [
    "## What Are Vision Transformers (ViTs)?\n",
    "Vision Transformers (ViTs) are a type of deep learning model that apply the **transformer architecture** to computer vision tasks **(segmentation, classification, detection)**. Instead of scanning images with convolutional filters (like CNNs), ViTs **split an image into small patches**, flatten them into vectors (tokens), and feed those tokens into a transformer encoder.\n",
    "\n",
    "Each token represents a region of the image, and **self-attention** layers allow the model to learn how different parts of the image relate to each other. \n",
    "\n",
    "This global attention mechanism often leads to better context understanding compared to local convolutions.\n",
    "\n",
    "**Key ideas behind ViTs:**\n",
    "- Images are divided into fixed-size patches (e.g., 16×16 pixels).\n",
    "- Each patch is embedded into a vector (a \"token\").\n",
    "- The transformer encoder processes all tokens simultaneously with multi-head self-attention.\n",
    "- The output is used for classification, detection, or other downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d32ef0-63b8-46e0-91af-446575a7178c",
   "metadata": {},
   "source": [
    "## Why Fine-Tune Vision Transformers?\n",
    "Pretrained vision transformers are trained on massive datasets (such as ImageNet).  \n",
    "Fine-tuning lets us **adapt** this general knowledge to a new, smaller dataset, allowing it to identify a specific subject.\n",
    "\n",
    "Fine-tuning updates model weights slightly so that it learns to specialize without starting from scratch.  \n",
    "However, ViTs have **millions of parameters**, which can be slow or memory-intensive to train fully, especially on CPUs (this environment). This is why we'll use Low Rank Adapters to fine tune the model.\n",
    "\n",
    "Inside a transformer, the **weights** that the model learns are stored in large **matrices**: grids of numbers that define how inputs are transformed into outputs. Updating all of these matrices during fine-tuning requires significant memory and compute.  \n",
    "\n",
    "By understanding that these weight matrices hold the model’s learned patterns, we can see how **LoRA** cleverly modifies only a *small portion* of them instead of the entire network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a4dd7-c733-425f-b14a-a5c5e188f29f",
   "metadata": {},
   "source": [
    "## What Is LoRA (Low-Rank Adaptation)?\n",
    "\n",
    "**At a high-level:**\n",
    "- **LoRA** is a **parameter-efficient fine-tuning (PEFT)** method that drastically reduces memory and compute needs.\n",
    "- Instead of updating all model weights, LoRA inserts small **trainable adapters** inside the attention layers.\n",
    "- These adapters learn low-rank updates to the original weights, while the base model remains frozen.\n",
    "- Simply put, we use **structures** (adapters) with less parameters to make updates to large matrices with many parameters.\n",
    "\n",
    "**Conceptually:**\n",
    "- Original weight matrix `W` → decomposed into `W + A·B`, where `A` and `B` are small, trainable low-rank matrices.\n",
    "- Only `A` and `B` are trained, often <1% of the total parameters.\n",
    "- If `W` is a matrix of `d x d` parameters, `A` and `B` are two, **much smaller** matrices that multiply to a `d x d` matrix so they can *update* `W`.\n",
    "    - We will get into how they work at a lower level later.\n",
    "- The model learns efficiently with little risk of overfitting.\n",
    "\n",
    "**Advantages:**\n",
    "- **Time**: Faster training (fewer trainable parameters).  \n",
    "- **Space**: Lower memory usage (ideal for limited hardware).  \n",
    "- **Modularity**: Easy to swap adapters to switch tasks (without retraining the base model).\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/0CHXduhqOiFHEX-JEeRwfQ/lora.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dd4d0-0db6-4f40-80a2-280931440efc",
   "metadata": {},
   "source": [
    "## Why This Approach Works\n",
    "By combining the **representational power of pretrained ViTs** with the **efficiency of LoRA**, we achieve the best of both worlds:\n",
    "- **Transfer** the general image understanding learned from large datasets.\n",
    "- **Efficiently** adapt to new, small datasets with minimal compute cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772491d8-7aab-40ea-9b72-7a5eebf33b96",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this project you will:\n",
    "- **Walkthrough the simple process of LoRA** in PyTorch.\n",
    "- **Load a pretrained Vision Transformer** (`DeiT Tiny` (5 million parameters).\n",
    "- **Load a dataset** of our subject and attach the label to each image.\n",
    "- **Extend the classifier head** to include our new class we want the model to be able to detect.\n",
    "- **Freeze all base model weights** so only the adapter layers are trained.\n",
    "- **Add LoRA adapters** to the attention projections (`query` and `value`).\n",
    "- **Fine-tune** on our small dataset using the Hugging Face `Trainer` module.\n",
    "- **Evaluate** the model on new images to measure its specialized performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7742e7-f491-46d4-b04f-1b3aebe675e3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "* `torch`: (PyTorch) library for building and training deep learning models.\n",
    "* `transformers`: (HuggingFace) library that provides pretrained transformer architectures and tools for model fine-tuning.\n",
    "* `datasets`: (HuggingFace) library for loading, processing, and managing datasets efficiently.\n",
    "* `peft`: (HuggingFace) Parameter-Efficient Fine-Tuning library that enables lightweight fine-tuning methods such as LoRA for large models.\n",
    "* `pillow`: Python Imaging Library (PIL) used for loading and manipulating image files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee8e08-b429-431e-b075-66345d2a01da",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Run the following cell to install all the necessary dependencies (**might take up to 10 minutes**):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12716087-ec1a-4bef-ba89-24f4008bf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.9.0 --index-url \"https://download.pytorch.org/whl/cpu\"\n",
    "!pip install transformers==4.57.1\n",
    "!pip install datasets==3.6.0\n",
    "!pip install peft==0.17.1\n",
    "!pip install pillow==12.0.0 \n",
    "!pip install matplotlib==3.10.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8e960-7891-4f00-afab-72f05bc40e68",
   "metadata": {},
   "source": [
    "## Principles of Training\n",
    "\n",
    "**If you understand gradient descent and loss functions, feel free to skip this section.**\n",
    "\n",
    "Before we do anything with LoRA or ViTs, let's take a minute to understand how parameters are trained at a high-level.\n",
    "\n",
    "First we define the function:\n",
    "$$\n",
    "\\hat{y} = F(x \\mid W)\n",
    "$$\n",
    "\n",
    "Where **x** is an input image and **W** is a set of parameters. The function $F$ might be a transformer or a neural network, the architectural details don't matter for now.\n",
    "\n",
    "When classifying an image, let's use image $x_1$:  \n",
    "$$\n",
    "F(x_1 \\mid W_1) = \\text{\"raccoon\"}\n",
    "$$\n",
    "\n",
    "\n",
    "However, for a second image $x_2$, which *is* also a raccoon, the model might output:\n",
    "$$\n",
    "F(x_2 \\mid W_1) = \\text{\"badger\"}\n",
    "$$\n",
    "\n",
    "\n",
    "This is a misclassification. Our goal is to adjust (train) the parameters so that the model makes fewer mistakes. After updating the weights to a new set $W_2$, we hope that:\n",
    "$$\n",
    "\\text{\"raccoon\"} = F(x_1 \\mid W_2) \\quad and \\quad \\text{\"raccoon\"} = F(x_2 \\mid W_2)\n",
    "$$\n",
    "\n",
    "The increased accuracy means less error and loss in our model.\n",
    "\n",
    "To increase accuracy and reduce error, we need to define a **loss function** that measures how \"wrong\" the classifier model is. \n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}(W))\n",
    "$$\n",
    "\n",
    "This is the general form of a loss function:\n",
    "- $L$ is the loss function itself.\n",
    "- $y$ is the true target, the correct class.\n",
    "- $\\hat{y}(W)$ is the model's predicted class as a function of the model's parameters, $W$.\n",
    "\n",
    "Training means finding the value of $W$ that **minimizes** this loss. The real math can get heavy, so here is a simple loss function example:\n",
    "$$\n",
    "L(w) = (w - 2)^2\n",
    "$$\n",
    "\n",
    "**Gradient descent** updates parameters by stepping in the direction that lowers the loss:\n",
    "$$\n",
    "w_{k+1} = w_k - \\eta \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "This is made possible because the loss function is a function of the model parameters, hence we can use a little bit of **calculus 1** and **chain rule** to find the **gradient** (derivative) of the loss and update the parameters in a direction according to that gradient.\n",
    "\n",
    "\n",
    "This ideally results in:\n",
    "\n",
    "$L(w_0) > L(w_1) > L(w_2)$\n",
    "\n",
    "In practice, real loss functions are far more complex, and the loss does not always decrease every step. In practice, $W$ is not a single number—it is a vector, a matrix, or even thousands of matrices. The goal of this lab is to show how we can minimize $L(y, \\hat{y}(W))$ without manually adjusting every value in $W$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91449a90-ddcd-4fd5-a34c-a566fe0164fd",
   "metadata": {},
   "source": [
    "## Simplified LoRA Process\n",
    "\n",
    "Before we get into fine tuning the actual vision model, let's go through the LoRA process with small matrices and simple matrix operations. You should have a basic knowledge of linear algebra and matrix manipulation for this.\n",
    "\n",
    "### Seed configuration\n",
    "\n",
    "Run the following cell to set the PyTorch seed so you'll get consistent results in the LoRA process visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c1439-f4ce-4593-a6e0-a5c464b0913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617c8bd-a50b-4d6f-b4d4-e3de8d1c4034",
   "metadata": {},
   "source": [
    "Let's start off with a base **weight matrix**, W. There exists many learnable weight matrices that store up to millions of values in a grid pattern. For this demo we'll work with a 5 x 6 (rows x columns) matrix with 30 *randomly* generated entries. If you set the `manual_seed`, you'll see the same matrix generated while the seed remains the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155a528-f5ed-4111-ae43-6ec89f5f6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 6\n",
    "out_features = 5\n",
    "\n",
    "W = torch.randn(out_features, in_features)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942a6cf-8bfa-4652-a56b-4ea5936dc393",
   "metadata": {},
   "source": [
    "The matrix `W` represents a linear transformation that maps an input vector of size `in_features` to an output vector of size `out_features`. In PyTorch we can demonstrate this by defining an input vector `x` and multiplying it by `W`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c470b0-c659-4cf6-8918-7990f212c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(in_features)\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = W @ x\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd384f9-0047-4d92-95f1-b750192c5dff",
   "metadata": {},
   "source": [
    "You can also take the **transpose** of `W` and multiply by `x`, it should output the same vector as `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50eb68-e458-4914-b104-0214672f226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ W.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b24895-ea2e-4b5e-b83f-c4a539d7188d",
   "metadata": {},
   "source": [
    "The purpose of **fine-tuning** is to make updates to `W` such that the output `y`, when applying the transformation `W` to `x`, is optimal for a given use case. In this case we could just update every parameter in `W`, after all it's only 30 values.\n",
    "\n",
    "If we wanted to add 1 to each parameter we could simply do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd59ad-b721-4aac-b40e-a58ee4154d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_update = torch.ones(out_features, in_features)\n",
    "print(\"W_update:\", W_update)\n",
    "\n",
    "W_updated = W + W_update\n",
    "print(\"W_updated:\", W_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721c4e2-2670-40f9-b452-7709dc58cfbb",
   "metadata": {},
   "source": [
    "The main idea of LoRA is that we don't change the big weight matrix $W$. Instead, we learn a small update called $ \\Delta W$. This update is much smaller than $W$, so it's faster and cheaper to train. \n",
    "\n",
    "Next, we will show how we can get a super small $BA$. It's not exactly as good as training $W$, but it's pretty close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921afc49-0703-45a7-b6e8-a3e4eb528778",
   "metadata": {},
   "source": [
    "In large transformer models, where `W` can contain millions or even billions of parameters, updating all of them is computationally expensive and memory-intensive.\n",
    "\n",
    "**Low-Rank Adapters** can make updates to these huge matrices while being much, much smaller (<1% of originalparameters).\n",
    "\n",
    "$$\n",
    "W' = W + \\frac{\\alpha}{r} \\cdot \\Delta W \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ;\n",
    "\\quad \\Delta W = A B\n",
    "$$\n",
    "\n",
    "- $W'$ is the updated weights\n",
    "- $W$ is the original base weights\n",
    "- $\\Delta W$ is a matrix with the same dimensions $W$ that updates it\n",
    "- $\\frac{\\alpha}{r}$ scales the impact of $\\Delta W$\n",
    "- $A$ and $B$ are the learnable **Low-Rank Adapters** that multiply to become $\\Delta W$\n",
    "\n",
    "We’ll skip the detailed math behind how the LoRA update formula is derived. See [**LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)**](https://arxiv.org/abs/2106.09685) for the full explanation.\n",
    "\n",
    "Before we create our LoRA (low-rank adapters) and use them to update `W`, we must discuss the key **hyperparameters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de952690-bdc8-4b72-b5e4-eb6a825bdf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 2\n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979bcd36-86d4-425c-a03c-fc67ce4d9ec2",
   "metadata": {},
   "source": [
    "### LoRA Hyperparameters\n",
    "\n",
    "LoRA introduces a few key hyperparameters that control how much capacity and influence the adapter has on the base model. There are other hyperparameters but these are the only two that must be configured.\n",
    "\n",
    "### Rank (`rank` or r)\n",
    "The rank determines the dimensionality of the low-rank update matrices, `A` and `B`. It controls how much expressive capacity the adapter has.\n",
    "- Low ranks (e.g. 4–8) → smaller, faster adapters, good for similar or simple tasks\n",
    "- Higher ranks (e.g. 32–64) → more capacity, useful for larger domain shifts\n",
    "- Parameter cost grows roughly linearly with rank\n",
    "- Mathematically, rank, is the number of linearly independent rows or columns in a matrix.\n",
    "\n",
    "### Alpha (`alpha` or α)\n",
    "The scaling factor that adjusts the strength of the LoRA update. It scales the low-rank matrix (`AB`) before adding it to the frozen weight.\n",
    "- Larger α → LoRA updates have more influence\n",
    "- Smaller α → more conservative, stable updates\n",
    "- Often set such that `alpha` / `rank` remains roughly constant across different ranks.\n",
    "    - `α′ = α / r` keeps the update scale roughly constant regardless of `r`\n",
    "    - e.g., `rank` = 2, `alpha` = 4, `alpha` / `rank` = 2 so when `rank` = 4, `alpha` = 8, `alpha` / `rank` = 2\n",
    " \n",
    "Now that we have our key hyperparameters, we can create our $A$ and $B$ low-rank adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6854cd-f872-46ea-8559-2df1479cf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(out_features, rank) * 0.1\n",
    "B = torch.randn(rank, in_features) * 0.1\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528b143-82c1-4457-8137-2d04abd59be3",
   "metadata": {},
   "source": [
    "For this demonstration, we'll use randomly generated values as the final adapters. In actual LoRA fine-tuning, the adapters are **randomly initialized** and then updated via [gradient descent](https://www.ibm.com/think/topics/gradient-descent?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Fine+Tune+DeiT+Tiny-v1_1762889552). Again, the primary difference to **full-parameter** fine-tuning is that fact that low-rank adapters have **less weights** and therefore requires **less compute and space** to update.\n",
    "\n",
    "The $\\Delta W$ is simply a matrix multiplication between the adapters ($AB$). This matrix has the same dimensions as the full $W$ matrix so it can be directly added.\n",
    "\n",
    "**This lab will NOT go into gradient descent as it is a large topic separate (but related) from LoRA fine-tuning. Feel free to learn about gradient descent through other guided projects on [Cognitive Class](https://cognitiveclass.ai/)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcec148-0990-47f3-b103-f99024e5248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_delta = A @ B\n",
    "print(\"W_delta:\", W_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0d667-70c7-4239-96d0-6a56e237fed8",
   "metadata": {},
   "source": [
    "Before adding the adapter matrix (`W_delta`), we scale it by $\\frac{\\alpha}{r}$. This scaling is implemented because as you increase `rank`, the adapters gain capacity. This increase in capacity causes the values in $AB$ to have a larger magnitude. \n",
    "\n",
    "Therefore, scaling by a factor of `1 / rank` keeps the **effect of rank balanced**. Mathematically, this means we take the average of each `rank` and scale `W_delta` by it. Finally, scaling by `alpha` gives you a control knob to the overall influence of the LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d1629-d869-472e-82da-11883b5da333",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lora = W + (alpha / rank) * W_delta\n",
    "print(\"W_lora:\", W_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1bd31-bb31-4405-947d-94fdb3e28d3a",
   "metadata": {},
   "source": [
    "The final `W_lora` ($W'$) matrix is the updated weights that took the time and compute of updating $A$ and $B$ to fine-tune. As mentioned previously, in transformers, the weights of $A$ and $B$ are often less than 1% of the weights of $W$, saving massive compute and space.\n",
    "\n",
    "Let's see a side-by-side comparison of the `y` output after training (`y_lora`) with before training (`y_base`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4c843-2920-4013-8e39-a9b20ba278f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lora = W_lora @ x\n",
    "\n",
    "print(\"y_lora:\", y_lora)\n",
    "print(\"y_base:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5232f6d-99e5-4269-b9a2-e4b9483aabdc",
   "metadata": {},
   "source": [
    "The tensors are similar, but there are **small changes** resulting from the LoRA adapter update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe6293-2201-4593-8722-6c9bf08dc3f1",
   "metadata": {},
   "source": [
    "# Fine-tuning a Vision Transformer (ViT)\n",
    "\n",
    "Now that we have a low-level understanding of how LoRA works, let's use it to fine-tune a **vision transformer** on an image dataset of raccoons for **classification**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c71cce-31ac-4969-8668-8a2b52c19e79",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all dependencies with the following code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6580a6-ec04-40bb-8ba9-7537561a42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the feature extractor and vision model modules from Hugging Face\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "# Import PIL for image loading and manipulation (e.g., resizing, opening JPEG/PNG)\n",
    "from PIL import Image\n",
    "# Load standard or custom datasets conveniently (e.g., from Hugging Face Hub or local files)\n",
    "from datasets import load_dataset, Dataset\n",
    "# Core PyTorch modules for building and using neural networks\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# OS module for file and directory operations (e.g., saving model checkpoints, logs)\n",
    "import os\n",
    "\n",
    "# LoraConfig sets LoRA hyperparameters, and get_peft_model wraps a model for efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Import classes for defining training configurations and managing the training loop with Hugging Face Trainer API\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd7711-68a0-4fe3-8a40-40af8f87e07f",
   "metadata": {},
   "source": [
    "## Fetch and Split Dataset\n",
    "\n",
    "**Add you own dataset if you want to fine-tune your own DeiT.**\n",
    "\n",
    "\n",
    "Next we'll fetch a dataset of 75 images of raccoons from a Github repository.\n",
    "- Best practice is to include a small number of images from other, visually related classes to provide contrastive supervision and prevent overconfidence toward the newly added label.\n",
    "- For this demo lab, we'll just stick to using a homogenous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d511518-36f1-4d9f-b08b-1d1019406337",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch trimmed --single-branch https://github.com/joshuazhou744/raccoon_dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ee3a6-7ef5-4e10-8868-30c858e10758",
   "metadata": {},
   "source": [
    "Let's extract the image file names into a list called `image_files`.\n",
    "We'll also shuffle the order of our images so our train and test sets are representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db314eab-b22c-4d00-8723-ca49db259fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./raccoon_dataset/images\"\n",
    "image_files = [\n",
    "    os.path.join(dataset_dir, f)\n",
    "    for f in os.listdir(dataset_dir)\n",
    "    if f.lower().endswith(\"jpg\")\n",
    "]\n",
    "\n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29a54f-f793-4fa4-b516-4a7aafa3fe8b",
   "metadata": {},
   "source": [
    "We'll manually create a **train-test split** with two different lists. The `train_files` will have 65 images while the `test_files` will contain the remaining 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584178c-fe63-49cf-a190-b5c22096de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = image_files[:65]\n",
    "test_files = image_files[65:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec03ac3-0c5b-412a-aea0-fe4266079d7d",
   "metadata": {},
   "source": [
    "A train-test split is a common method of **cross-validation** to evaluate the output of a model on unseen test data after training on the train data.\n",
    "\n",
    "Let's create a helper function, `load_images`, to convert each image file into a PIL (Python Imaging Library) Image that can be stored in a HuggingFace `Dataset` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a725c8-81f5-4bca-b8f4-29777c823b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_paths):\n",
    "    return {\"image\": [Image.open(f).convert(\"RGB\") for f in file_paths]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad53979-62aa-4bd3-913e-05784ef5fc2c",
   "metadata": {},
   "source": [
    "We'll use the `Dataset.from_dict` method to convert each `load_images` dictionary to a `Dataset` object. Notice that we leave our datasets **unlabeled**, this is because all images in our dataset have the **same label**.\n",
    "\n",
    "Since every training example shares the same label, there’s no need to assign it individually during image loading. Instead, we’ll attach the label later in one step using the `Dataset.map()` method, after the model configuration is updated with our new class label. This ensures that the numeric label ID we assign matches the model’s internal label mapping. This will make sense in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30f1ce-baae-499a-a648-3462ac5b67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(load_images(train_files))\n",
    "test_dataset = Dataset.from_dict(load_images(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9de12-323e-4702-84d8-165293d97c1e",
   "metadata": {},
   "source": [
    "Let's take a look at one of our images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3734a-b7e6-4a1a-9616-69ee1d8df167",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95a3f9-b166-4eff-83df-713c9c4b68e6",
   "metadata": {},
   "source": [
    "## Vision Transformer\n",
    "\n",
    "For this project, we'll fine-tune a Data-efficient Image Transformer (DeiT) Tiny model with around **5 million parameters**. Compared to LLMs (billions of parameters), this is a miniscule amount. As such, the less overall parameters in a vision transformer make it viable to fine-tune in an environment without a GPU (this environment, which runs on CPU on a Cloud Container and is displayed to you here).\n",
    "\n",
    "The model ID on HuggingFace is `facebook/deit-tiny-patch16-224`.\n",
    "- The model was trained on the **ImageNet-1k dataset** (1 million images, 1,000 classes).\n",
    "- The model trains and infers on **224x224 pixel** images, all images are resized to this resolution before handled.\n",
    "- Each image is divided into **16x16 patches** (each patch is 14x14 pixels), which are interpreted as input tokens by the transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57216374-6d69-4110-9bd1-5259d758f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ViTImageProcessor.from_pretrained(\"facebook/deit-tiny-patch16-224\")\n",
    "base_model = ViTForImageClassification.from_pretrained(\"facebook/deit-tiny-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d2818-3205-43c9-b9bf-24e2aaa5ac74",
   "metadata": {},
   "source": [
    "### Update the Classification Layer\n",
    "\n",
    "Before any fine-tuning, we have to update the labels the model recognizes. As mentioned above, the base vision model can detect 1,000 classes, so we'll have to add a **raccoon** class and make that 1,001 classes.\n",
    "\n",
    "We'll have to update the **linear classifier head** and the configuration dictionaries (`id2label` and `label2id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5c5cf-4eb9-453b-8590-9fc3aed06b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define old linear head and it's number of input features\n",
    "old_head = base_model.classifier\n",
    "in_features = old_head.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e44c58-7ce7-478b-b918-4655d763a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360867f5-de4e-4409-9531-5a65cf46ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b93ab-d793-42c8-ae19-cf6fc5a8f471",
   "metadata": {},
   "source": [
    "Let's update the **Classifier Head** that maps output feature vectors into probabilities for each class.\n",
    "- First we get `old_head`, a `Linear` layer that maps feature vectors to logits (probabilities).\n",
    "- For DeiT, `in_features` or the length of the feature vector per batch is 192, and there are 1,000 logits as output for the base classifier head (`old_head`)\n",
    "    - Logits can be interpreted as \"confidence scores\" corresponding to a class.\n",
    "    - More specifically, the classifier head is responsible for mapping a matrix of shape (`batch_size`, 192) embeddings to a matrix of shape (`batch_size`, 1,000) logits for each class per sample.\n",
    "        - `batch_size` is the number of samples (images) per batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e2dfc-89ba-4057-a336-b42ed42e1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new head\n",
    "new_num_labels = old_head.out_features + 1\n",
    "base_model.classifier = nn.Linear(in_features, new_num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5702f5c-3b61-473c-9497-cb3c27b2a46b",
   "metadata": {},
   "source": [
    "Now we want the (linear) classifier head to be able to map to **1,001** classes (raccoon class included).\n",
    "\n",
    "We initialized the classifier head with a new `Linear` layer shaped (`in_features`, `new_num_labels`) or (192, 1001). These values are randomly initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618fb92-35e5-4238-9f97-eecee1fc17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab84f39-8421-4a33-957c-e5277f347e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da886a6-742c-4fc0-ae04-c0c5012bb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda8ccb-5970-47d0-897b-79ccad28daaa",
   "metadata": {},
   "source": [
    "Let's copy over the weights and biases of the original classes (first 1,000) from the `old_head`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a188e7c-3b67-4aa1-9011-484810ec4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy old weights/biases into the first 1000 rows, initialize the last neuron (row)\n",
    "with torch.no_grad():\n",
    "    base_model.classifier.weight[:old_head.out_features] = old_head.weight\n",
    "    base_model.classifier.bias[:old_head.out_features] = old_head.bias\n",
    "    nn.init.normal_(base_model.classifier.weight[old_head.out_features:], std=0.02)\n",
    "    nn.init.zeros_(base_model.classifier.bias[old_head.out_features:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d676f-3a8d-4f98-aa27-68f7092a3f76",
   "metadata": {},
   "source": [
    "- This keeps the **original 1,000 ImageNet weights** intact so you don't lose the general vision knowledge the model already has.\n",
    "- The new row (1,001st neuron) gets normalized weights with a standard deviation of 0.02 and a bias of zero.\n",
    "    - This makes it so the model will learn that class from scratch during fine-tuning.\n",
    "- `with torch.no_grad()` temporarily disables PyTorch gradient tracking while copying and initializing weights/biases.\n",
    "    - These are manual operations that shouldn't be recorded in the **computation graph**.\n",
    "    - The **computation graph** is the internal structure to track operations to a tensor so gradients can be automatically calculated during training (loss minimization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c85c28-07c3-4063-95c9-57e96f2203cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e4035-1d3d-47a6-a0f1-6614c45f226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bebef-6e59-4d8d-9ede-6c7011d78c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7f464-960b-4069-af13-25a62280134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.classifier.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ccb6be-8ae8-424c-8619-8739f1ff419b",
   "metadata": {},
   "source": [
    "### Interpreting the Classifier Layer\n",
    "\n",
    "The attention layers of a transformer are complex and have multiple weight matrices to manipulate and operate on. They are not the main focus of this project so we won't get too in-depth on them. However, we will talk about the **Linear Classfication Layer** as it is crucial for mapping learned feature representations from the transformer into label/class predictions.\n",
    "\n",
    "Mathematically, the layer performs the operation (assuming 1 sample per batch):\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "- $x$ = input vector (shape = [`in_features`], [192])\n",
    "- $W$ = weight matrix (shape = [`out_features`, `in_features`], [1001, 192])\n",
    "    - Intuitively, each row of $W$ corresponds to the output features (192) of the label of row index (0-1000)\n",
    "    - e.g., Row 0 $\\to$ weights that determine how to detect class 0 (`id2label[0]`)\n",
    "- $b$ = bias vector (shape = [`out_features`], [1001])\n",
    "    - Shifts the entire output of one neuron (row), moving the **decision boundary** and allowing for **activation thresholds.**\n",
    "- $y$ = logits vector (shape=[`out_features`], [1001]), a tensor of \"confidence scores\" for each associated label\n",
    "\n",
    "Simply speaking, each row of $W$ shows the **learned features** of a specific class. During inference, the classification head takes the input feature vector $x$ from the transformer encoder and computes the dot product between $x$ and each class's weight vector (a row in $W$). The resulting logits $y$ measures the **similarity** of the input's features with each class's learned features, higher values mean stronger matches. We'll see how the loss function turns these logits into probabilities near the end of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b54999-c9f1-4955-92dd-326e7b6e95cc",
   "metadata": {},
   "source": [
    "### Update the Label-ID Conversion\n",
    "\n",
    "IDs are integers that correspond to a label, they are required to calculate how close the model's predicted probability distribution is to the true (one-hot) target distribution. \n",
    "\n",
    "For example:\n",
    "- The classifier head maps the final output's features (192) -> logits (1,000, one for each label).\n",
    "    - e.g., `logits = [2,3, -1.5, 0.7]` and `true_label = 0`\n",
    "    - An integer class ID is required so the loss function knows which logit index corresponds to the true class during loss computation.\n",
    "- **After** the classifier head outputs the logits, the loss function, `CrossEntropyLoss`, does two things to the logits:\n",
    "    - Normalizes the logits into probabilities into a softmax function:\n",
    "$$\n",
    "p_i = \\frac{e^{logit_i}}{\\sum_{j}e^{logit_j}}\n",
    "$$\n",
    "    - Finds the probability of the correct class (`true_class`) and computes loss using **negative log-likelihood**:\n",
    "$$\n",
    "loss = -\\log(p_{true\\:class})\n",
    "$$\n",
    "    - Reminder: **Loss is computed after the classifier head**.\n",
    "\n",
    "\n",
    "Let's update the dictionaries used to convert between label and ID in the model configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b65400-f177-4970-bece-8c6ae1432d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy original labels\n",
    "id2label = base_model.config.id2label.copy()\n",
    "label2id = base_model.config.label2id.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4625b-23aa-4da9-83bd-2a328082cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cf929-05e8-4e5c-a811-45c607dd953f",
   "metadata": {},
   "source": [
    "First we create a shallow copy of the model's original `id2label` and `label2id` dictionaries. This is done to avoid **mutating the model's configuration directly** since `model.config` is shared internally by many components. By copying first, we can safely modify and reassign the mappings.\n",
    "\n",
    "After copying, you should see that there are 1,000 IDs (0-999) and labels, one for each **ImageNet-1k** class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8ee73-8784-45d3-8503-4df93aa06d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_head.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ff195-a4f3-47d1-b734-c7097ae43e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"raccoon\"\n",
    "\n",
    "# update config label mappings\n",
    "id2label[old_head.out_features] = LABEL\n",
    "label2id[LABEL] = old_head.out_features\n",
    "\n",
    "base_model.config.num_labels = new_num_labels\n",
    "base_model.config.id2label = id2label\n",
    "base_model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd59673-ea63-4c36-ab08-f3d90d9ed4a8",
   "metadata": {},
   "source": [
    "Now we add the new `LABEL`, **raccoon**. \n",
    "- We'll take the next available integer ID (1,000) and assign it the label in the `id2label` dictionary. We'll also do the reverse for `label2id`.\n",
    "- Finally, we update the model configurations with the new information and mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41201f00-21d3-4f48-bc81-7fa19ec70a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f402e-8b48-40c9-a1a4-f1de9bd4cd82",
   "metadata": {},
   "source": [
    "Quick sanity check that the label was added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26683b0-b101-4470-87c6-02e6417d9725",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Before we train, we need to label all our data. This made is easy with our newly configured mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92013b1-c8bd-4e29-84cc-d5f035587bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset.map(\n",
    "    lambda ex: {\"labels\": base_model.config.label2id[LABEL]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae52b73-094c-451f-ae96-ec9f71bdf67a",
   "metadata": {},
   "source": [
    "We simply map the `train_dataset` and attach the ID to a `labels` field for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df386c-4e5f-495c-9074-c97d3014ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(examples):\n",
    "    inputs = extractor(examples[\"image\"], return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(examples[\"labels\"])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24a3d6-84eb-4151-829b-a8372ee92bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45605c35-b031-472f-bbdb-57a45ba9f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633abfa1-7f0b-496c-b480-a7b38916c574",
   "metadata": {},
   "source": [
    "After attaching a label, we batch and preprocess the data for model training.\n",
    "\n",
    "We use the **feature extractor** (loaded earlier) to convert each raw PIL image into the normalized pixel tensors expected by the DeiT model.\n",
    "To review, the extractor:\n",
    "- Resizes images to **224x224** pixels.\n",
    "- Normalizes pixel values to match the scale of ImageNet images.\n",
    "- Converts pixel values to PyTorch tensors\n",
    "\n",
    "Again, we use the `.map()` method to apply the `preprocess_batch` helper function to each batch of images.\n",
    "- The `batched=True` field tells the `datasets` library to pass multiple examples to the function at once rather than one at a time.\n",
    "    - Makes preprocessing faster and more efficient.\n",
    "- The `remove_columns` parameter removes the original dataset columns (`image` and `labels`) after preprocessing.\n",
    "    - Processed dataset only keeps fields returned by `preprocess_batch` (`pixel_values` and `labels`) which are model-ready tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3a6fb-b504-4942-b529-9a70c919cf62",
   "metadata": {},
   "source": [
    "## Selecting LoRA Target Layers\n",
    "\n",
    "Before we begin training, let's define the **LoRA configuration**. Specifically, let's determine which parts of the model we’ll adapt.\n",
    "\n",
    "The DeiT model contains **12 transformer encoder layers** (numbered 0–11).  \n",
    "Each encoder layer includes three key linear projections within the attention mechanism: **query (Q)**, **key (K)**, and **value (V)**.  \n",
    "\n",
    "We’ll target the **query** and **value** projections for LoRA adaptation because they directly control how information flows through the attention mechanism:  \n",
    "- **Query (Q):** Determines which features each token *attends to*.\n",
    "    - *Where* the model looks.\n",
    "- **Value (V):** Determines *what information* is passed along once attention is applied.\n",
    "    - *What* the model learns.\n",
    "\n",
    "We avoid modifying **Key (K)** because it defines how tokens are represented for attention matching. Changing both **Q** and **K** simultaneously could introduce conflicting updates and destabilize training.\n",
    "\n",
    "Again, this project won't go in depth on **multi-headed attention** as it's not the primary focus, feel free to explore more on your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e2058-4da8-484e-be7b-0cc7da22a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in base_model.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6150933-b434-4b7d-95f5-0b5049502287",
   "metadata": {},
   "source": [
    "This cell should output a long list of layer names and their types. You should see each attention head as `vit.encoder.layer.n` where `n` is an integer from 0 - 11. Each attention layer should have a **query , key, and value** layer.\n",
    "\n",
    "With 12 attention layers and target 2 modules, **query** and **value**, in each attention layer, we'll create and train **24** total LoRA.\n",
    "\n",
    "You should also see the last layer at the end is our `classifier` layer that maps the final feature vector to logits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3684f-0f28-4254-8831-8c371cc35a1b",
   "metadata": {},
   "source": [
    "### LoRA Process Example\n",
    "\n",
    "Let's go through the same manual LoRA process we did at the beginning applied to a real **Query Linear Layer** in the ViT.\n",
    "\n",
    "First we'll define `q_layer` as the Query Linear Layer of the first (index 0) Attention Layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141e0b5-e923-49c6-9f03-fd6e41242eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer = base_model.vit.encoder.layer[0].attention.attention.query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faacf8-8715-4621-b4fc-50a7861256b7",
   "metadata": {},
   "source": [
    "Each `Linear`layer consists of two learnable components, a **weight** matrix and a **bias** vector. These should be familiar from when we did the **Simplified LoRA Process** at the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416809e-0738-4107-b845-5c84cb2b271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = q_layer.weight\n",
    "b = q_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6afc6b-572f-484e-9c5a-7aedd834db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3a4c7-ff30-4c7e-8c85-c9ee68063f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8778d-d31c-4f5e-a712-362d0234600d",
   "metadata": {},
   "source": [
    "This linear layer performs the simple operation:\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the output vector.\n",
    "- $x$ is the input vector.\n",
    "- $W^T$ is the transposed weight matrix.\n",
    "- $b$ is the bias vector.\n",
    "\n",
    "Let's apply this formula to a random input vector, `x`, and get an output vector, `y_base`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63638912-9664-4e6e-90f1-21c914a405ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 192)\n",
    "y_base = x @ W.T + b\n",
    "y_base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42425a86-158f-4580-8cdc-1a91b5851894",
   "metadata": {},
   "source": [
    "As explained before, our LoRA creates $A$ and $B$ that updates $W$ to a $W'$ by the formula:\n",
    "\n",
    "$$\n",
    "W' = W + \\frac{\\alpha}{r} \\cdot \\Delta W \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ ;\n",
    "\\quad \\Delta W = A B\n",
    "$$\n",
    "\n",
    "Let's create our adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0b286-40c8-4c7a-8cd5-3052e33b139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 4\n",
    "alpha = 8\n",
    "\n",
    "A = torch.randn(W.shape[0], rank) * 0.01\n",
    "B = torch.randn(rank, W.shape[1]) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79947a-69f3-4150-85c1-6624f02257de",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cddda-5758-488c-9632-cb8c95be3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9024e-2469-4712-a471-ac2d7b404927",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A @ B).shape == W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b988e-7b80-4eae-b7e0-07702487f34b",
   "metadata": {},
   "source": [
    "With our LoRA ($A$, $B$) made, we can create our updated weight matrix, `W_updated`, and calculate a new output vector, `y_lora`, and compare it to `y_base`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d4fb8-8bed-4687-b42a-60c2540e0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_updated = W + (alpha / rank) * (A @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d5f58-5e22-40d5-ac32-a29673c7ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lora = x @ W_updated.T + b\n",
    "y_lora.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc646aa3-fd5b-4760-86c7-a7386236b57c",
   "metadata": {},
   "source": [
    "`y_lora` and `y_base` should be the same shape, let's compare their first 5 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012549c-5499-49f4-8c22-184a12d2da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_base: \", y_base[0][:5])\n",
    "print(\"y_lora: \", y_lora[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2b848-419f-4e39-b6ac-f6762f807568",
   "metadata": {},
   "source": [
    "You should see small differences as a result of the updated weight matrix from the LoRA.\n",
    "\n",
    "Before we begin training, we'll **freeze all of the model’s parameters**.  \n",
    "- Freezing means setting `requires_grad = False` for each parameter, which prevents it from being updated during backpropagation.  \n",
    "- This is important because we’re using **LoRA (Low-Rank Adaptation)**.\n",
    "    - We want to fine-tune only the small, added adapter layers, while keeping the pretrained model’s original weights fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bb142-a6a2-4924-99d5-ffb7b0a291b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze model weights\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfda628-7c51-44ff-a623-224c876c0af6",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Now let's create LoRA for the entire transformer by defining a `LoraConfig` object and attaching them to the `base_model` with the `get_peft_model` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14bac3-069f-4c5d-a713-56c0e666210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    modules_to_save=[\"classifier\"]\n",
    ")\n",
    "\n",
    "# attach lora to the base vit\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd26bc-cdb3-4489-bfc2-437d9d6a59d8",
   "metadata": {},
   "source": [
    "We'll use these parameters for `LoraConfig`:\n",
    "- `r`: rank of LoRA adapters.\n",
    "- `lora_alpha`: alpha scaling value.\n",
    "- `lora_dropout`: percent (0.2 is 20%) of activations (output features) to zero out to prevent overfitting.\n",
    "- `bias`: how to handle biases, `none` means biases are frozen and untrained.\n",
    "- `target_modules`: `query` and `value` mean we attach LoRA for these modules in each attention layer.\n",
    "- `task_type`: informs the PEFT library how to correctly wrap, save, and reload adapters for a classification-style model. (e.g., )\n",
    "- `modules_to_save`: these modules are made trainable and saved at the final checkpoint, since we modified the `classifier` layer, we need to make it trainable.\n",
    "    - **No LoRA are attached to the classifier head because it's already small as is**.\n",
    "    - Instead, it is trained directly with standard gradient descent for the new label.\n",
    "\n",
    "The following cell sets the weights of the **classifier linear layer** to trainable parameters, performing the same action as adding `classifier` to `modules_to_save`. You can run the cell as a sanity check to ensure the classifier head learns our new label \"raccoon\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a4ac0-d0db-4a60-b8d1-1b1fddc9b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the new classifier head is trainable\n",
    "for n, p in model.named_parameters():\n",
    "    if \"classifier\" in n:\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbe2a0-ee0d-4c2c-baa4-06abeff7fa4d",
   "metadata": {},
   "source": [
    "Let's take a quick look at the impact of LoRA. Since the base model weights are frozen (`requires_grad=False`), they aren't trainable. Run the following cell to print out the makeup of trainable parameters to total parameters in the model with attached LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cb579-3335-4a4a-a1e5-fc242d162d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a summary of trainable vs frozen params\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be333019-c91a-43e3-bff3-6c7143859b9a",
   "metadata": {},
   "source": [
    "The fraction of trainable parameters compared to total parameters should be a tiny fraction. This shows the effectiveness of LoRA fine-tuning that lets us **update huge models with lightweight adapters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1f43c-9fcf-48f6-8d6d-22f14c710d45",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let's create our `TrainingArguments` object to define the configurations for the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad283de-1d61-4f30-b9fd-1da592a1a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=6e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=2,\n",
    "    save_total_limit=1,\n",
    "    fp16=False,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1e5e4-8f54-425d-8790-0b6ee3837561",
   "metadata": {},
   "source": [
    "- `output_dir`: where to save model checkpoints, logs, and the final trained model (only trained adapters, **NOT** including base model).\n",
    "- `per_device_train_batch_size`: batch size per GPU/CPU, `2` samples are processed in one forward/backward pass.\n",
    "- `gradient_accumulation_steps`: how many forward/backward passes to accumulate before updating one optimizer update that updates **ALL** trainable weights.\n",
    "    - **Effective Batch Size**: `per_device_train_batch_size` x `gradient_accumulation_steps` = $2 * 4$ = $8$ samples processed per weight update.\n",
    "- `num_train_epochs`: number of times we iterate a full pass over the entire training dataset.\n",
    "- `learning_rate`: initial scaling factor of each weight update used by the optimizer, a scheduler may change the learning rate during training.\n",
    "- `weight_decay`: regularization to penalize large weight values to reduce overfitting, used by the optimizer during the optimizer update.\n",
    "- `logging_steps`: display loss at every **nth** step, we'll display the loss at every second step.\n",
    "- `save_total_limit`: keeps the last **n** checkpoint(s) to the `output_dir`, we only save the latest to conserve disk space.\n",
    "- `fp16`: use half-precision floating points (16-bit), set to `False` because CPUs **do NOT** support efficient FP16 compute and only perform efficient compute in FP32 (32-bit).\n",
    "- `seed`: randomness seed for reproducibility of dropout patterns, dataset shuffling, and weight initialization.\n",
    "\n",
    "Now let's define the trainer with all of our training components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207379c-327a-46db-8965-7423926e1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=processed_dataset,\n",
    "    processing_class=extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8bc44-340b-4bf8-b4b8-506e159261a8",
   "metadata": {},
   "source": [
    "- `model`: the PEFT model itself (`base_model` with attached LoRA).\n",
    "- `args`: training arguments and configuration from above.\n",
    "- `train_dataset`: the processed and formatted dataset.\n",
    "- `processing_class`: an optional processor to process input data, in our case it's an **image extractor** that doesn't do much because our dataset is **already preprocessed** into `labels` and `pixel_values`.\n",
    "    - It can exist as metadata, be used in evaluation methods if raw images are provided, or restore consistent training when resuming training with a non-preprocessed dataset.\n",
    "\n",
    "Now let's call the `train` method to begin training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce71846-5cb3-4b44-8854-fd037958bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26ffd5-cf61-4fd4-b192-f263ff2a233a",
   "metadata": {},
   "source": [
    "You should see a progress bar with details and loss periodically printed out (every 2 steps).\n",
    "\n",
    "You should see the loss initially start at around 6-7 and reduce down to around 1. We'll evaluate the fine-tuned model and you can retrain or continue training after.\n",
    "\n",
    "The model (LoRA and `modules_to_save` so the classifier head) can be saved to a destination folder with the following method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a95bb4-53a0-46ab-aa8b-b75d4cbc2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./raccoon_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fdd6a-badf-4f5a-a6bb-5f8f19c05fb9",
   "metadata": {},
   "source": [
    "This is typically done to make LoRA modular and flexible for different use case. We'll use everything in-memory but keep this in mind when using LoRA in other situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721128b-4ccc-47cd-a73d-e7fec3037e2d",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Before we find the average loss using the `test_dataset` to evaluate our model, let's configure our workspace and the model itself for evaluation.\n",
    "- The `model.eval()` method disables `lora_dropout` that occurs during training.\n",
    "    - Prevents random portions of activations from being set to zero, ensuring stable predictions during evaluation.\n",
    "- Setting the `model.to(\"cpu\")` ensures all model parameters are placed on the CPU, matching the device where input tensors (images) will be processed.\n",
    "    - Prevents device mismatch between parameters and input tensors.\n",
    "    - This environment only has CPU so it's a good check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e66a90-565b-42a7-8f30-5af6e9dd2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# ensure all of our model weights and computations are on the CPU, redundant but good safety check\n",
    "device = \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd322cc-cee3-4e27-8719-fcc4825e65a0",
   "metadata": {},
   "source": [
    "Before we start the validation loop, let's initialize some tracking variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692277b-c730-4c8d-bc31-fa3ae45dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "n = len(test_dataset)\n",
    "raccoon_index = model.config.label2id[LABEL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82db52-0aaa-4439-9d64-17758cf0f194",
   "metadata": {},
   "source": [
    "- `total_loss` is the cumulative sum of all losses.\n",
    "- `n` is the number of samples in the `test_dataset`.\n",
    "    - `average_loss` will be computed by `total_loss` / `n`.\n",
    "- `raccoon_index` is the integer index and ID of the target class in the model's output layer.\n",
    "    - This index corresponds to the place of the logit and probability of the **correct** class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0e035-a782-44e8-bda9-a2a46201cfde",
   "metadata": {},
   "source": [
    "### Validation Loop\n",
    "\n",
    "We determine the average loss of our model by computing the **negative log-likelihood (NLL)** of the probability for each sample in `test_dataset`. First, we derive label probabilities (normal distribution) by applying a softmax to the output logits.\n",
    "\n",
    "$$\n",
    "p = softmax(logits)\n",
    "$$\n",
    "\n",
    "- Logits are values that represent the \"relative evidence for a label\", they can be negative, positive, zero, anything, and they do not sum to 1.\n",
    "    - Hence, they **cannot** be interpreted as probabilities.\n",
    "- Probabilities are the result of applying a softmax function to logits.\n",
    "    - They normalize logits to a **categorical probability distribution** such that:\n",
    "\n",
    "$$\n",
    "p_i \\ge 0 \\quad and \\quad \\sum_i p_i = 1\n",
    "$$\n",
    "\n",
    "The softmax function is:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ are logits.\n",
    "- $p_i$ are normalized probabilities.\n",
    "\n",
    "Now that we have probabilities from 0 - 1, we can use the negative log-likelihood of the true class to compute the loss.\n",
    "\n",
    "$$\n",
    "loss = -\\log(p_{true})\n",
    "$$\n",
    "\n",
    "And we'll accumulate this loss in a `total_loss` counter to get `avg_loss` in the end.\n",
    "\n",
    "### `CrossEntropyLoss`\n",
    "\n",
    "- In our validation loop, we are manually computing the same loss value that `CrossEntropyLoss` would return.\n",
    "- `CrossEntropyLoss` is a very common loss technique because it:\n",
    "    - Works extremely well with **multi-class classification.**\n",
    "    - **Penalizes** wrong predictions heavily.\n",
    "    - Combines **softmax and NLL** into one stable operation.\n",
    "\n",
    "\n",
    "Let's loop through each image in `test_dataset` and accumulate the `total_loss` of all test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also classify our predictions in this step into these classes:\n",
    "- True Positive (TP): image is a raccoon and the model predicts raccoon\n",
    "- False Positive (FP): image is not a raccoon but the model predicts raccoon\n",
    "- False Negative (FN): image is a raccoon but the model predicts not raccoon\n",
    "- True Negative (TN): image is not a raccoon and the model predicts not raccoon\n",
    "\n",
    "Positive class = raccoon \\\n",
    "Negative class = not raccoon\n",
    "\n",
    "We can use these counts to get the following evaluation metrics:\n",
    "- Accuracy: Correct predictions / Total predictions (fraction of predictions that were correct)\n",
    "    - $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "- Precision: Correct raccoon predictions / Total raccoon predicitons (how often predicted positives are actually correct)\n",
    "    - $\\frac{TP}{TP + FP}$\n",
    "- Recall: Correct raccoon predictions / Total raccoon images (how many actual positives were correctly found)\n",
    "    - $\\frac{TP}{TP + FN}$\n",
    "- F-1: Harmonic mean of precision and recall (how well the model balances missing positives and raising false alarms)\n",
    "    - $\\frac{2 * precision * recall}{precision + recall}$\n",
    "\n",
    "**NOTE: since the test dataset only has raccoon images, TN = FP = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = fp = fn = tn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfda75-b2f5-4646-b450-868d93652cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in test_dataset:\n",
    "    # get image\n",
    "    image = example['image']\n",
    "    # extract image to pixel_values (tensors that represent images)\n",
    "    inputs = extractor(images=image, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    # disable gradient tracking which increases speed and reduces memory for inference\n",
    "    with torch.no_grad():\n",
    "        # pass in all keyword arguments of the inputs (pixel_values)\n",
    "        outputs = model(**inputs)\n",
    "        # get the logits \n",
    "        logits = outputs.logits\n",
    "        # apply softmax to normalize logits into probabilities\n",
    "        # dim=-1 because classes are stored in the last index of the logits\n",
    "        # logits shape = [batch, patches, classes]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # predicted class\n",
    "    pred_label = torch.argmax(probs, dim=-1).item()\n",
    "    true_label = raccoon_index  # all test images are raccoons in this lab\n",
    "\n",
    "    # evaluation metrics (not loss)\n",
    "    if pred_label == true_label:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "    # get the probability of the image being raccoon\n",
    "    p_true = probs[0, raccoon_index]\n",
    "    # add the negative log of the probability of raccoon to the total_loss\n",
    "    # tiny addition 1e-8 to prevent log(0)\n",
    "    total_loss += -torch.log(p_true + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fdd27-f940-424d-960c-907676b8a9dd",
   "metadata": {},
   "source": [
    "Let's take a look at some of the intermediate structures we encountered in the validation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea39f1a-65f9-422e-9452-c644251dbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467458e0-f819-4a6e-856e-eb4ac47698ff",
   "metadata": {},
   "source": [
    "The `pixel_values` of the input image tensors are of shape `[batch_size, channels, height, width]` which is the standard format for PyTorch images.\n",
    "- 1 sample (image) per batch.\n",
    "- 3 color channels (RGB).\n",
    "- Images are of 224 x 224 pixels (height x width) resized by the `extractor` to the fixed resolution the model recognizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbae83-44bc-4ad8-bd76-5962bb4d9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0aa719-cfae-4164-b69f-f91a06218b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439bbc6-2208-4436-8894-4e8d65abb83d",
   "metadata": {},
   "source": [
    "Both the `logits` and `probs` should have the same shape as the softmax function only rescales values rather than mapping to different spaces.\n",
    "\n",
    "Each element in these vectors (one-dimensional tensor) corresponds to the model's predicted score (logit) or normalized probability for the **specific class index**.\n",
    "- The probability we care about is at the **1001st index** because that probability corresponds to the probability of the model predicting the image as a raccoon, which we know is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23270fe6-f35b-4687-890f-970efb38beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = total_loss / n\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634df65-debc-4ae2-a6ca-1aab8c6a15f4",
   "metadata": {},
   "source": [
    "Finally we calculate the average loss for each image in the `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy  = tp / (tp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "f1        = (\n",
    "    2 * precision * recall / (precision + recall)\n",
    "    if (precision + recall) > 0 else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1:        {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate our other evluation metrics here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505b57e-0e10-4a4b-85b5-1108ae898916",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The last step of this project is to use our fine-tuned model to classify some images!\n",
    "\n",
    "First let's create a function to get the top 5 predictions, we'll use the same processes as above to get the probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103f059-0749-4f31-8bc1-31ff02292a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5(path: str):\n",
    "    # load image from path\n",
    "    img = Image.open(path)\n",
    "    \n",
    "    # extract image to the proper format for the model\n",
    "    inputs = extractor(images=img, return_tensors=\"pt\")\n",
    "    \n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # get top 5 preds\n",
    "    top5 = torch.topk(probs, k=5)\n",
    "    # print each label and their probability\n",
    "    for i, score in zip(top5.indices[0], top5.values[0]):\n",
    "        label = model.config.id2label[int(i)]\n",
    "        prob = f\"{score.item():.4f}\"\n",
    "        print(f\"{label}  {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5e9d5-276b-48fe-85e3-a34d0c39e655",
   "metadata": {},
   "source": [
    "After we get the `probs` vector, we take the **top 5** probabilities and print them out for us to see what the model predicts the image as.\n",
    "\n",
    "Let's go through a few examples, first run the following cell to download some image for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59feafb-4037-4dcc-abb4-c15b11cb240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Wm3R4qVcaLTEYGIFClNnkA/raccoon.jpg\"\n",
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/jo8-H5RsPJwM4-Nu1hFNbQ/dog.jpg\"\n",
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/3EZaa89vfWdCVBS7drf4iQ/cat.jpg\"\n",
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/x-7eM1p4HaaOjrLyEYxLWw/badger.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00e23e-3e34-468d-8b22-ae84ef9c8bba",
   "metadata": {},
   "source": [
    "Now you'll see four new files in your file sidebar (on the left): **badger.jpg, cat.jpg, dog.jpg, and raccoon.jpg**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e4e19-8466-4952-9711-abf44f86169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top5(\"cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13553921-b3c5-4e41-84da-31a207466ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top5(\"dog.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38224cb2-ceda-42e8-9c72-df3274da963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top5(\"badger.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579b9f3-1b24-46dc-979d-eac05d71ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top5(\"raccoon.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bc044-8da1-4b1b-90be-bcf3afa9839e",
   "metadata": {},
   "source": [
    "Getting the top 5 predictions for each should show you the accuracy of our fine-tuned model.\n",
    "\n",
    "Feel free to **upload images** of your own using the **Upload Files** button at the top of the file sidebar to infer other images yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce8460-2c2b-4d3c-b47b-d1ab9a31d64d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this project, you **fine-tuned** a Vision Transformer (DeiT Tiny) using **Low-Rank Adapters (LoRA)** to train a pretrained image classification model for detecting a new class, **raccoons**. Instead of updating millions of parameters in the base model, LoRA injects lightweight adapter layers into the **query and value** projections of each transformer attention block. These tensors were trained and updated while the rest of the model remained frozen, allowing for fast and memory-efficient fine-tuning even without a GPU.\n",
    "\n",
    "Throughout the project, you learned how to:\n",
    "- **Prepare and preprocess** image datasets for vision transformers.\n",
    "- Modify and **extend the classifier head** to add a new class.\n",
    "- Update the model’s internal **label-ID mappings.**\n",
    "- Use a **LoRA** configuration to target specific transformer modules.\n",
    "- Fine-tune the model using the **Hugging Face Trainer API.**\n",
    "- Evaluate model performance using **logits, softmax, and negative log-likelihood.**\n",
    "- Run inference and inspect the **top-k predictions.**\n",
    "\n",
    "This workflow is fully customizable and easily extended, you can:\n",
    "- Swap in **your own dataset** (any image collection you want the model to classify).\n",
    "- Modify the LoRA **hyperparameters** (`r, lora_alpha, lora_dropout, target modules`).\n",
    "- Change **training configurations** such as `batch size, learning rate, epochs, or weight decay`.\n",
    "- Explore fine-tuning on tasks **beyond classification** (e.g., segmentation, retrieval, multi-head outputs).\n",
    "\n",
    "By customizing these components, you can push the model toward different behaviors, performance levels, or domains.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "prev_pub_hash": "1663cff09bfd7c8a376f81b440d049f2c236d19373bdff9b19c9455a9dc54930"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
